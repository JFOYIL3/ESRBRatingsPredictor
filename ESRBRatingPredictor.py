# -*- coding: utf-8 -*-
"""Project478.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uMF2q5Vto5YsUhxFh-EJzidU4fxHCCBQ
"""

from google.colab import files
import io
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from pandas.core.common import random_state
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import StandardScaler
import warnings
from sklearn.decomposition import PCA
import time

warnings.filterwarnings("ignore")

# upload our file
uploaded = files.upload()
uploaded = files.upload()
train = pd.read_csv('esrb_train.csv')
test = pd.read_csv('esrb_test.csv')

train.drop(columns=["title"], inplace=True)
test.drop(columns=["title"], inplace=True)

X_train = train[train.columns[:-1]]
X_test = test[test.columns[:-1]] 
y_train = train[["esrb_rating"]]
y_test = test[["esrb_rating"]]

# holdout validation
# split again
X_train_new, X_valid, y_train_new, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=123)

# we want to store the scores
# of all the models
# to determine the best model
model_scores = {}

# KNN
print("Training KNN (without Normalization)")

# values of k
K = [1, 3, 5, 7, 9, 11, 13, 15, 17]

# metrics
metric = ["manhattan", "chebyshev", "hamming"]

# weights
weight = ["uniform", "distance"]

# calculate the best hyperparameters
start = time.time()
results = {}
for k in K:
  for m in metric:
    for w in weight:
      clf = KNeighborsClassifier(n_neighbors=k, metric=m, weights=w)
      clf.fit(X_train_new, y_train_new.values.ravel())
      pred = clf.predict(X_valid)
      # store the score in a dict
      # we will use f1-score
      results[k, m, w] = f1_score(y_valid, pred, average="weighted")

# print the best hyperparameters
max_key = max(results, key=results.get)
best_k = max_key[0]
best_metric = max_key[1]
best_weight = max_key[2]
best_neigh = KNeighborsClassifier(n_neighbors=best_k, metric=best_metric, weights=best_weight)
best_neigh.fit(X_train_new, y_train_new)
best_pred = best_neigh.predict(X_test)

# print the reports
end = time.time()
print("Training Time:", end-start)
print("The best hyperparameters for KNN is K = {}, metric = {}, and weight = {}.".format(best_k, best_metric, best_weight))
print("\nTesting Accuracy:", accuracy_score(y_test, best_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, best_pred))
print("\nClassification Report:\n", classification_report(y_test, best_pred, digits=4))

# store the score for this model
model_scores["KNN"] = f1_score(y_test, best_pred, average="weighted")
print("-----------------------------------------------------------")

# DecisionTreeClassifier
print("Training DecisionTreeClassifier (without Normalization)")

# criterion
criterion = ['gini','entropy']

# max_depth
max_depth = [4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]

# calculate the best hyperparameters
start = time.time()
results = {}
for c in criterion:
  for depth in max_depth:
    clf = DecisionTreeClassifier(criterion=c, max_depth=depth)
    clf.fit(X_train_new, y_train_new.values.ravel())
    pred = clf.predict(X_valid)
    # store the score in a dict
    # we will use f1-score
    results[c, depth] = f1_score(y_valid, pred, average="weighted")

# print the best hyperparameters
max_key = max(results, key=results.get)
best_c = max_key[0]
best_depth = max_key[1]
best_neigh = DecisionTreeClassifier(criterion=best_c, max_depth=best_depth)
best_neigh.fit(X_train_new, y_train_new)
best_pred = best_neigh.predict(X_test)

# print the reports
end = time.time()
print("Training Time:", end-start)
print("The best hyperparameters for DecisionTreeClassifier is criterion = {} and max_depth = {}.".format(best_c, best_depth))
print("\nTesting Accuracy:", accuracy_score(y_test, best_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, best_pred))
print("\nClassification Report:\n", classification_report(y_test, best_pred, digits=4))

# store the score for this model
model_scores["DecisionTreeClassifier"] = f1_score(y_test, best_pred, average="weighted")
print("-----------------------------------------------------------")

# LogisticRegression
print("Training LogisticRegression (without Normalization)")

# C
C = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]

# solver
solver = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']

# class_weight
class_weight = ['balanced', None]

# calculate the best hyperparameters
start = time.time()
results = {}
for c in C:
  for s in solver:
    for w in class_weight:
      if s == "newton-cg":
        newton_penalty = ['l2', 'none']
        for p in newton_penalty:
          clf = LogisticRegression(penalty=p, C=c, solver=s, class_weight=w)
          clf.fit(X_train_new, y_train_new.values.ravel())
          pred = clf.predict(X_valid)
          # store the score in a dict
          # we will use f1-score
          results[p, c, s, w] = f1_score(y_valid, pred, average="weighted")
      elif s == "lbfgs":
        lbfgs_penalty = ['l2', 'none']
        for p in lbfgs_penalty:
          clf = LogisticRegression(penalty=p, C=c, solver=s, class_weight=w)
          clf.fit(X_train_new, y_train_new.values.ravel())
          pred = clf.predict(X_valid)
          # store the score in a dict
          # we will use f1-score
          results[p, c, s, w] = f1_score(y_valid, pred, average="weighted")
      elif s == "liblinear":
        liblinear_penalty = ['l1', 'l2']
        for p in liblinear_penalty:
          clf = LogisticRegression(penalty=p, C=c, solver=s, class_weight=w)
          clf.fit(X_train_new, y_train_new.values.ravel())
          pred = clf.predict(X_valid)
          # store the score in a dict
          # we will use f1-score
          results[p, c, s, w] = f1_score(y_valid, pred, average="weighted")
      elif s == "sag":
        sag_penalty = ['l2', 'none']
        for p in sag_penalty:
          clf = LogisticRegression(penalty=p, C=c, solver=s, class_weight=w)
          clf.fit(X_train_new, y_train_new.values.ravel())
          pred = clf.predict(X_valid)
          # store the score in a dict
          # we will use f1-score
          results[p, c, s, w] = f1_score(y_valid, pred, average="weighted")
      elif s == "saga":
        saga_penalty = ['elasticnet', 'l1', 'l2']
        for p in saga_penalty:
          if p == 'elasticnet':
            clf = LogisticRegression(penalty=p, C=c, solver=s, class_weight=w, l1_ratio=0.5)
            clf.fit(X_train_new, y_train_new.values.ravel())
            pred = clf.predict(X_valid)
            # store the score in a dict
            # we will use f1-score
            results[p, c, s, w] = f1_score(y_valid, pred, average="weighted")
          else:
            clf = LogisticRegression(penalty=p, C=c, solver=s, class_weight=w)
            clf.fit(X_train_new, y_train_new.values.ravel())
            pred = clf.predict(X_valid)
            # store the score in a dict
            # we will use f1-score
            results[p, c, s, w] = f1_score(y_valid, pred, average="weighted")
               
# print the best hyperparameters
max_key = max(results, key=results.get)
best_p = max_key[0]
best_c = max_key[1]
best_s = max_key[2]
best_weight = max_key[3]
best_neigh = LogisticRegression(penalty=best_p, C=best_c, solver=best_s, class_weight=best_weight)
best_neigh.fit(X_train_new, y_train_new)
best_pred = best_neigh.predict(X_test)

# print the reports
end = time.time()
print("Training Time:", end-start)
print("The best hyperparameters for LogisticRegression is penalty = {}, C = {}, solver = {}, and class_weight = {}.".format(best_p, best_c, best_s, best_weight))
print("\nTesting Accuracy:", accuracy_score(y_test, best_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, best_pred))
print("\nClassification Report:\n", classification_report(y_test, best_pred, digits=4))

# store the score for this model
model_scores["LogisticRegression"] = f1_score(y_test, best_pred, average="weighted")
print("-----------------------------------------------------------")

# NaiveBayes
print("Training NaiveBayes (without Normalization)")


start = time.time()
clf = GaussianNB()
clf.fit(X_train, y_train)
pred = clf.predict(X_test)

# print the reports
end = time.time()
print("Training Time:", end-start)
print("\nTesting Accuracy:", accuracy_score(y_test, pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, pred))
print("\nClassification Report:\n", classification_report(y_test, pred, digits=4))

# store the score for this model
model_scores["NaiveBayes"] = f1_score(y_test, pred, average="weighted")
print("-----------------------------------------------------------")

from sklearn.svm import SVC

# SVC
print("Training SVC (without Normalization)")

# C
C = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]

# kernel
kernels = ["linear", "poly", "rbf", "sigmoid"]

# gamma
gammas = ["scale", "auto"]

# calculate the best hyperparameters
start = time.time()
results = {}
for c in C:
  for k in kernels:
    for g in gammas:
      clf = SVC(C=c, kernel=k, gamma=g)
      clf.fit(X_train_new, y_train_new.values.ravel())
      pred = clf.predict(X_valid)
      # store the score in a dict
      # we will use f1-score
      results[c, k, g] = f1_score(y_valid, pred, average="weighted")

# print the best hyperparameters
max_key = max(results, key=results.get)
best_c = max_key[0]
best_k = max_key[1]
best_g = max_key[2]
best_neigh = SVC(C=best_c, kernel=best_k, gamma=best_g)
best_neigh.fit(X_train_new, y_train_new)
best_pred = best_neigh.predict(X_test)

# print the reports
end = time.time()
print("Training Time:", end-start)
print("The best hyperparameters for SVC is C = {}, kernel = {}, and gamma = {}.".format(best_c, best_k, best_g))
print("\nTesting Accuracy:", accuracy_score(y_test, best_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, best_pred))
print("\nClassification Report:\n", classification_report(y_test, best_pred, digits=4))

# store the score for this model
model_scores["SVC"] = f1_score(y_test, best_pred, average="weighted")
print("-----------------------------------------------------------")

from sklearn.svm import LinearSVC

penalties = [ 'l1', 'l2']
loss = ['hinge', 'squared_hinge']
C = [0.1, 0.3, 0.5, 0.7, 0.8, 1]


# calculate the best hyperparameters
start = time.time()
results = {}
for c in C:
  for p in penalties:
    if p == "l1":
      l = "squared_hinge"
      d = False
    else:
      l = "hinge"
      d = True
    clf = LinearSVC(penalty=p, C=c, loss=l, dual=d)
    clf.fit(X_train_new, y_train_new.values.ravel())
    pred = clf.predict(X_valid)
    # store the score in a dict
    # we will use f1-score
    results[p, c, l] = f1_score(y_valid, pred, average="weighted")
      
               
# print the best hyperparameters
max_key = max(results, key=results.get)
best_p = max_key[0]
best_c = max_key[1]
best_l = max_key[2]
if best_p == "l1":
  d = False
else:
  d = True
best_neigh = LinearSVC(penalty=best_p, C=best_c, loss=best_l, dual=d)
best_neigh.fit(X_train_new, y_train_new)
best_pred = best_neigh.predict(X_test)

# print the reports
end = time.time()
print("Training Time:", end-start)
print("The best hyperparameters for LinearSVC is penalty = {}, C = {}, loss = {}.".format(best_p, best_c, best_l))
print("\nTesting Accuracy:", accuracy_score(y_test, best_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, best_pred))
print("\nClassification Report:\n", classification_report(y_test, best_pred, digits=4))

# store the score for this model
model_scores["LinearSVC"] = f1_score(y_test, best_pred, average="weighted")
print("-----------------------------------------------------------")

from sklearn.neural_network import MLPClassifier

activations = ['identity', 'logistic', 'tanh', 'relu']
solvers = ['lbfgs', 'sgd', 'adam']

# holdout validation for MLPClassifier
X_train_MLP, X_valid_MLP, y_train_MLP, y_valid_MLP = train_test_split(X_train, y_train, test_size=0.2, random_state=123)

start = time.time()
results = {}
for a in activations:
  for s in solvers:
    clf = MLPClassifier(hidden_layer_sizes=(100, 100), activation=a, solver=s)
    clf.fit(X_train_new, y_train_new.values.ravel())
    pred = clf.predict(X_valid)
    # store the score in a dict
    # we will use f1-score
    results[a, s] = f1_score(y_valid, pred, average="weighted")
      
               
# print the best hyperparameters
max_key = max(results, key=results.get)
best_a = max_key[0]
best_s = max_key[1]
best_neigh = MLPClassifier(hidden_layer_sizes=(100, 100), activation=best_a, solver=best_s)
best_neigh.fit(X_train_new, y_train_new)
best_pred = best_neigh.predict(X_test)

# print the reports
end = time.time()
print("Training Time:", end-start)
print("The best hyperparameters for MLPClassifier is activation = {}, solver = {}.".format(best_a, best_s))
print("\nTesting Accuracy:", accuracy_score(y_test, best_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, best_pred))
print("\nClassification Report:\n", classification_report(y_test, best_pred, digits=4))

# store the score for this model
model_scores["MLP"] = f1_score(y_test, best_pred, average="weighted")
print("-----------------------------------------------------------")

from sklearn.ensemble import RandomForestClassifier

n_estimators = {25, 50, 75, 100, 125, 150, 175, 200}
criterion = {"gini", "entropy"}

results = {}

start = time.time()
for n in n_estimators:
  for c in criterion:
    clf = RandomForestClassifier(n_estimators=n, criterion=c, random_state=123)
    clf.fit(X_train_new, y_train_new)
    pred = clf.predict(X_valid)
    results[n, c] = f1_score(y_valid, pred, average="weighted")

# print the best hyperparameters
max_key = max(results, key=results.get)
best_n = max_key[0]
best_c = max_key[1]
best_neigh = RandomForestClassifier(n_estimators=best_n, criterion=best_c, random_state=123)
best_neigh.fit(X_train_new, y_train_new)
best_pred = best_neigh.predict(X_test)

end = time.time()
print("Training Time:", end-start)
print("The best hyperparameters for RandomForestClassifier is n_estimator = {} and criterion = {}.".format(best_n, best_c))
print("\nTesting Accuracy:", accuracy_score(y_test, best_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, best_pred))
print("\nClassification Report:\n", classification_report(y_test, best_pred, digits=4))

# store the score for this model
model_scores["RandomForestClassifier"] = f1_score(y_test, best_pred, average="weighted")
print("-----------------------------------------------------------")

# Our best Model (not normalized)
max_key = max(model_scores, key=model_scores.get)
print("The best model for our data without normalization is {} with an f1 score of: {}".format(max_key, model_scores[max_key]))
print("-----------------------------------------------------------")
print("-----------------------------------------------------------")

# Normalize our Data
print("Normalizing Data")
model_scores_norm = {}

from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import OrdinalEncoder

scaler = StandardScaler()
X_train_norm = scaler.fit_transform(X_train)
X_test_norm = scaler.transform(X_test)

# KNN Normalized
print("Training KNN (with Normalization)")

# values of k
K = [1, 3, 5, 7, 9, 11, 13, 15, 17]

# metrics
metric = ["manhattan", "chebyshev", "hamming"]

# weights
weight = ["uniform", "distance"]

# calculate the best hyperparameters
start = time.time()
results = {}
for k in K:
  for m in metric:
    for w in weight:
      neigh = KNeighborsClassifier(n_neighbors=k, metric=m, weights=w)
      neigh.fit(X_train_norm, y_train)
      pred = neigh.predict(X_test_norm)
      results[k, m, w] = f1_score(y_test, pred, average="weighted")

# print the best hyperparameters
max_key = max(results, key=results.get)
best_k = max_key[0]
best_metric = max_key[1]
best_weight = max_key[2]
best_neigh = KNeighborsClassifier(n_neighbors=best_k, metric=best_metric, weights=best_weight)
best_neigh.fit(X_train_norm, y_train)
best_pred = best_neigh.predict(X_test_norm)

# print the reports
end = time.time()
print("Training Time:", end-start)
print("The best hyperparameters for KNN Normalized is K = {}, metric = {}, and weight = {}.".format(best_k, best_metric, best_weight))
print("\nTesting Accuracy:", accuracy_score(y_test, best_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, best_pred))
print("\nClassification Report:\n", classification_report(y_test, best_pred, digits=4))

# store the score for this model
model_scores_norm["KNN_NORM"] = f1_score(y_test, best_pred, average="weighted")
print("-----------------------------------------------------------")

# DecisionTree Normalized
print("Training DecistionTreeClassifier (with Normalization)")

# criterion
criterion = ['gini','entropy']

# max_depth
max_depth = [4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]

# calculate the best hyperparameters
start = time.time()
results = {}
for c in criterion:
  for depth in max_depth:
    clf = DecisionTreeClassifier(criterion=c, max_depth=depth)
    clf.fit(X_train_norm, y_train)
    pred = clf.predict(X_test_norm)
    # store the score in a dict
    # we will use f1-score
    results[c, depth] = f1_score(y_test, pred, average="weighted")

# print the best hyperparameters
max_key = max(results, key=results.get)
best_c = max_key[0]
best_depth = max_key[1]
best_neigh = DecisionTreeClassifier(criterion=best_c, max_depth=best_depth)
best_neigh.fit(X_train_norm, y_train)
best_pred = best_neigh.predict(X_test_norm)

# print the reports
end = time.time()
print("Training Time:", end-start)
print("The best hyperparameters for DecisionTreeClassifier is criterion = {} and mex_depth = {}.".format(best_c, best_depth))
print("\nTesting Accuracy:", accuracy_score(y_test, best_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, best_pred))
print("\nClassification Report:\n", classification_report(y_test, best_pred, digits=4))

# store the score for this model
model_scores_norm["DecisionTreeClassifier_NORM"] = f1_score(y_test, best_pred, average="weighted")
print("-----------------------------------------------------------")

# Logistic Regression Normalized
print("Training LogisticRegression (with Normalization)")

# C
C = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]

# solver
solver = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']

# class_weight
class_weight = ['balanced', None]

# calculate the best hyperparameters
start = time.time()
results = {}
for c in C:
  for s in solver:
    for w in class_weight:
      if s == "newton-cg":
        newton_penalty = ['l2', 'none']
        for p in newton_penalty:
          clf = LogisticRegression(penalty=p, C=c, solver=s, class_weight=w)
          clf.fit(X_train_norm, y_train)
          pred = clf.predict(X_test_norm)
          # store the score in a dict
          # we will use f1-score
          results[p, c, s, w] = f1_score(y_test, pred, average="weighted")
      elif s == "lbfgs":
        lbfgs_penalty = ['l2', 'none']
        for p in lbfgs_penalty:
          clf = LogisticRegression(penalty=p, C=c, solver=s, class_weight=w)
          clf.fit(X_train_norm, y_train)
          pred = clf.predict(X_test_norm)
          # store the score in a dict
          # we will use f1-score
          results[p, c, s, w] = f1_score(y_test, pred, average="weighted")
      elif s == "liblinear":
        liblinear_penalty = ['l1', 'l2']
        for p in liblinear_penalty:
          clf = LogisticRegression(penalty=p, C=c, solver=s, class_weight=w)
          clf.fit(X_train_norm, y_train)
          pred = clf.predict(X_test_norm)
          # store the score in a dict
          # we will use f1-score
          results[p, c, s, w] = f1_score(y_test, pred, average="weighted")
      elif s == "sag":
        sag_penalty = ['l2', 'none']
        for p in sag_penalty:
          clf = LogisticRegression(penalty=p, C=c, solver=s, class_weight=w)
          clf.fit(X_train_norm, y_train)
          pred = clf.predict(X_test_norm)
          # store the score in a dict
          # we will use f1-score
          results[p, c, s, w] = f1_score(y_test, pred, average="weighted")
      elif s == "saga":
        saga_penalty = ['elasticnet', 'l1', 'l2']
        for p in saga_penalty:
          if p == 'elasticnet':
            clf = LogisticRegression(penalty=p, C=c, solver=s, class_weight=w, l1_ratio=.5)
            clf.fit(X_train_norm, y_train)
            pred = clf.predict(X_test_norm)
            # store the score in a dict
            # we will use f1-score
            results[p, c, s, w] = f1_score(y_test, pred, average="weighted")
          else:
            clf = LogisticRegression(penalty=p, C=c, solver=s, class_weight=w)
            clf.fit(X_train_norm, y_train)
            pred = clf.predict(X_test_norm)
            # store the score in a dict
            # we will use f1-score
            results[p, c, s, w] = f1_score(y_test, pred, average="weighted")
               
# print the best hyperparameters
max_key = max(results, key=results.get)
best_p = max_key[0]
best_c = max_key[1]
best_s = max_key[2]
best_weight = max_key[3]
best_neigh = LogisticRegression(penalty=best_p, C=best_c, solver=best_s, class_weight=best_weight, l1_ratio=.5)
best_neigh.fit(X_train_norm, y_train)
best_pred = best_neigh.predict(X_test_norm)

# print the reports
end = time.time()
print("Training Time:", end-start)
print("The best hyperparameters for LogisticRegression is penalty = {}, C = {}, solver = {}, and class_weight = {}.".format(best_p, best_c, best_s, best_weight))
print("\nTesting Accuracy:", accuracy_score(y_test, best_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, best_pred))
print("\nClassification Report:\n", classification_report(y_test, best_pred, digits=4))

# store the score for this model
model_scores_norm["LogisticRegression_NORM"] = f1_score(y_test, best_pred, average="weighted")
print("-----------------------------------------------------------")

# Naive Bayes Normalized
print("Training NaiveBayes (with Normalization)")
start = time.time()

clf = GaussianNB()
clf.fit(X_train_norm, y_train)
pred = clf.predict(X_test_norm)

# print the reports
end = time.time()
print("Training Time:", end-start)
print("\nTesting Accuracy:", accuracy_score(y_test, pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, pred))
print("\nClassification Report:\n", classification_report(y_test, pred, digits=4))

# store the score for this model
model_scores_norm["NaiveBayes_NORM"] = f1_score(y_test, pred, average="weighted")
print("-----------------------------------------------------------")

from sklearn.svm import SVC

# SVC Normalized
print("Training SVC (with Normalization)")

# C
C = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]

# kernel
kernels = ["linear", "poly", "rbf", "sigmoid"]

# gamma
gammas = ["scale", "auto"]

# calculate the best hyperparameters
start = time.time()
results = {}
for c in C:
  for k in kernels:
    for g in gammas:
      clf = SVC(C=c, kernel=k, gamma=g)
      clf.fit(X_train_norm, y_train.values.ravel())
      pred = clf.predict(X_test_norm)
      # store the score in a dict
      # we will use f1-score
      results[c, k, g] = f1_score(y_test, pred, average="weighted")

# print the best hyperparameters
max_key = max(results, key=results.get)
best_c = max_key[0]
best_k = max_key[1]
best_g = max_key[2]
best_neigh = SVC(C=best_c, kernel=best_k, gamma=best_g)
best_neigh.fit(X_train_norm, y_train)
best_pred = best_neigh.predict(X_test_norm)

# print the reports
end = time.time()
print("Training Time:", end-start)
print("The best hyperparameters for SVC is C = {}, kernel = {}, and gamma = {}.".format(best_c, best_k, best_g))
print("\nTesting Accuracy:", accuracy_score(y_test, best_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, best_pred))
print("\nClassification Report:\n", classification_report(y_test, best_pred, digits=4))

# store the score for this model
model_scores_norm["SVC_NORM"] = f1_score(y_test, best_pred, average="weighted")
print("-----------------------------------------------------------")

from sklearn.svm import LinearSVC

# LinearSVC Normalized
penalties = [ 'l1', 'l2']
loss = ['hinge', 'squared_hinge']
C = [0.1, 0.3, 0.5, 0.7, 0.8, 1]


# calculate the best hyperparameters
start = time.time()
results = {}
for c in C:
  for p in penalties:
    if p == "l1":
      l = "squared_hinge"
      d = False
    else:
      l = "hinge"
      d = True
    clf = LinearSVC(penalty=p, C=c, loss=l, dual=d)
    clf.fit(X_train_norm, y_train.values.ravel())
    pred = clf.predict(X_test_norm)
    # store the score in a dict
    # we will use f1-score
    results[p, c, l] = f1_score(y_test, pred, average="weighted")
      
               
# print the best hyperparameters
max_key = max(results, key=results.get)
best_p = max_key[0]
best_c = max_key[1]
best_l = max_key[2]
if best_p == "l1":
  d = False
else:
  d = True
best_neigh = LinearSVC(penalty=best_p, C=best_c, loss=best_l, dual=d)
best_neigh.fit(X_train_norm, y_train)
best_pred = best_neigh.predict(X_test_norm)

# print the reports
end = time.time()
print("Training Time:", end-start)
print("The best hyperparameters for LinearSVC is penalty = {}, C = {}, loss = {}.".format(best_p, best_c, best_l))
print("\nTesting Accuracy:", accuracy_score(y_test, best_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, best_pred))
print("\nClassification Report:\n", classification_report(y_test, best_pred, digits=4))

# store the score for this model
model_scores_norm["LinearSVC_NORM"] = f1_score(y_test, best_pred, average="weighted")
print("-----------------------------------------------------------")

from sklearn.neural_network import MLPClassifier

activations = ['identity', 'logistic', 'tanh', 'relu']
solvers = ['lbfgs', 'sgd', 'adam']

# holdout validation for MLPClassifier
X_train_MLP, X_valid_MLP, y_train_MLP, y_valid_MLP = train_test_split(X_train, y_train, test_size=0.2, random_state=123)

start = time.time()
results = {}
for a in activations:
  for s in solvers:
    clf = MLPClassifier(hidden_layer_sizes=(100, 100), activation=a, solver=s)
    clf.fit(X_train_norm, y_train.values.ravel())
    pred = clf.predict(X_test_norm)
    # store the score in a dict
    # we will use f1-score
    results[a, s] = f1_score(y_test, pred, average="weighted")
      
               
# print the best hyperparameters
max_key = max(results, key=results.get)
best_a = max_key[0]
best_s = max_key[1]
best_neigh = MLPClassifier(hidden_layer_sizes=(100, 100), activation=best_a, solver=best_s)
best_neigh.fit(X_train_norm, y_train)
best_pred = best_neigh.predict(X_test_norm)

# print the reports
end = time.time()
print("Training Time:", end-start)
print("The best hyperparameters for MLPClassifier is activation = {}, solver = {}.".format(best_a, best_s))
print("\nTesting Accuracy:", accuracy_score(y_test, best_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, best_pred))
print("\nClassification Report:\n", classification_report(y_test, best_pred, digits=4))

# store the score for this model
model_scores_norm["MLP_NORM"] = f1_score(y_test, best_pred, average="weighted")
print("-----------------------------------------------------------")

from sklearn.ensemble import RandomForestClassifier

n_estimators = {25, 50, 75, 100, 125, 150, 175, 200}
criterion = {"gini", "entropy"}

results = {}

start = time.time()
for n in n_estimators:
  for c in criterion:
    clf = RandomForestClassifier(n_estimators=n, criterion=c, random_state=123)
    clf.fit(X_train_norm, y_train)
    pred = clf.predict(X_test_norm)
    results[n, c] = f1_score(y_test, pred, average="weighted")

# print the best hyperparameters
max_key = max(results, key=results.get)
best_n = max_key[0]
best_c = max_key[1]
best_neigh = RandomForestClassifier(n_estimators=best_n, criterion=best_c, random_state=123)
best_neigh.fit(X_train_norm, y_train)
best_pred = best_neigh.predict(X_test_norm)

end = time.time()
print("Training Time:", end-start)
print("The best hyperparameters for RandomForestClassifier is n_estimator = {} and criterion = {}.".format(best_n, best_c))
print("\nTesting Accuracy:", accuracy_score(y_test, best_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, best_pred))
print("\nClassification Report:\n", classification_report(y_test, best_pred, digits=4))

# store the score for this model
model_scores_norm["RandomForestClassifier_NORM"] = f1_score(y_test, best_pred, average="weighted")
print("-----------------------------------------------------------")

# Our best Model (normalized)
max_key = max(model_scores_norm, key=model_scores_norm.get)
print("The best model for our data with normalization is {} with an f1 score of: {}".format(max_key, model_scores_norm[max_key]))
print("-----------------------------------------------------------")

# best non-normalized model
max_key = max(model_scores, key=model_scores.get)

# best normalized model
max_key_normal = max(model_scores_norm, key=model_scores_norm.get)

# compare the two models
final_models = {max_key: model_scores[max_key],
              max_key_normal: model_scores_norm[max_key_normal]}
best_model = max(final_models, key=final_models.get)
print("{}: {}".format(best_model, final_models[best_model]))

print(model_scores)
print(model_scores_norm)

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split
from keras.layers import Dropout, Activation, Input
from keras.layers.core.dense import Dense
import numpy as np
from keras.layers.convolutional import Conv2D
from keras.layers.convolutional import MaxPooling2D
from keras.layers import Flatten  
from keras.datasets import mnist
from keras.models import Sequential
from sklearn.preprocessing import LabelEncoder
from keras.utils import np_utils
from keras.regularizers import l2

encoder = LabelEncoder()
encoder.fit(y_train)
encoded_y_train = encoder.transform(y_train)
encoder = LabelEncoder()
encoder.fit(y_test)
encoded_y_test = encoder.transform(y_test)
dummy_y_train = np_utils.to_categorical(encoded_y_train)
dummy_y_test = np_utils.to_categorical(encoded_y_test)



keras.backend.clear_session()

model = Sequential()

model.add(Input(shape=(32,)))
model.add(Dense(16, activation='relu'))

model.add(Dense(8, activation='relu'))

model.add(Dense(4, activation='relu'))

model.add(Dense(2, activation='relu'))

model.add(Dense(4, activation='sigmoid'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()

# fit the keras model on the dataset
model.fit(X_train_norm, dummy_y_train, epochs=10, batch_size=5, validation_split=.2)

pred = model.predict(X_test_norm)
print("\n")
print(classification_report(np.argmax(dummy_y_test, axis=1), np.argmax(pred, axis=1), digits=4))